{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c90752-def4-401e-88f4-b4e33bea8a5c",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "593d59f1-d849-4dca-aef0-943b331f5aac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m                   \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m\u001b[38;5;250m            \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mae\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import datetime\n",
    "import tensorflow                       as tf\n",
    "from tensorflow.keras.callbacks         import TensorBoard\n",
    "from tensorflow.keras.layers            import Input, Lambda, Conv2D,Dropout,MaxPooling2D,Conv2DTranspose,concatenate,BatchNormalization, Activation\n",
    "from tensorflow.keras.models            import Model\n",
    "from tensorflow.keras.optimizers        import Adam\n",
    "from keras.utils                        import plot_model\n",
    "from tensorflow.keras                   import layers, models\n",
    "from tensorflow.keras.losses            import mae\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import random, time\n",
    "\n",
    "import skimage                      as ski\n",
    "from   skimage.filters              import threshold_otsu\n",
    "from   skimage                      import io, color\n",
    "from   skimage.color                import rgb2gray\n",
    "from   skimage                      import filters\n",
    "import cv2                          as cv\n",
    "import matplotlib.pyplot            as plt\n",
    "import gc \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddaabf3-c20a-40c5-a48a-1b6077197b00",
   "metadata": {},
   "source": [
    "### 2. Load directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8918bc28-81a7-49d9-bc9a-25b3f1bf0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main directory \n",
    "mainPath = \"/home/ppgi/Trabajo/Codigos_generate_data/DLCODES-VER-5\"\n",
    "\n",
    "directory= os.path.abspath(mainPath)\n",
    "\n",
    "# Add directory to sys.path\n",
    "sys.path.append(directory)\n",
    "\n",
    "# import script where parameters are defined\n",
    "import param  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6b6d8-e340-49c7-90a7-0a6e6af984ea",
   "metadata": {},
   "source": [
    "### 3. Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f41e0f-8816-46e6-96bd-ad8b558c60b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => /home/ppgi/Trabajo/Codigos_generate_data/DLCODES-VER-5/Geometry\n",
      "1 => /home/ppgi/Trabajo/Codigos_generate_data/DLCODES-VER-5/Magnitude\n",
      "2 => /home/ppgi/Trabajo/Codigos_generate_data/DLCODES-VER-5/Pression\n",
      "3 => /home/ppgi/Trabajo/Codigos_generate_data/DLCODES-VER-5/U001\n",
      "4 => /home/ppgi/Trabajo/Codigos_generate_data/DLCODES-VER-5/U002\n",
      "\n",
      "Train = 0.8, Test = 0.2, Valid = 0.0\n",
      "\n",
      "Dimensions resized h = 128, w = 256\n",
      "\n",
      "Pack into 100 images\n"
     ]
    }
   ],
   "source": [
    "# Vizualizancing main directories \n",
    "list_of_paths = param.list_paths \n",
    "\n",
    "# Defining amount of train,valid,test images   \n",
    "n_train=param.n_train;    n_valid=param.n_valid;    n_test=param.n_test\n",
    "\n",
    "# Defining a number of samples\n",
    "n_sample=param.n_sample\n",
    "\n",
    "# For resizing dimentions\n",
    "h=param.img_height\n",
    "w=param.img_width\n",
    "\n",
    "#Pack them into batches (we’ll use batches of 10 images e.g.)\n",
    "BATCH_SIZE=param.pack_size \n",
    "\n",
    "'''\n",
    "Showing parameters\n",
    "'''\n",
    "for i,j in enumerate(list_of_paths):\n",
    "    print(f'{i} => {j}')\n",
    "\n",
    "print(f'\\nTrain = {n_train}, Test = {n_test}, Valid = {n_valid}\\n')\n",
    "print(f'Dimensions resized h = {h}, w = {w}\\n')\n",
    "print(f'Pack into {BATCH_SIZE} images')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fee28e5-4bee-4f0d-b50e-4102e72404a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_in_batches(img_dir, batch_size=500, target_shape=(128, 128)):\n",
    "    filenames = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    n_files = len(filenames)\n",
    "    \n",
    "    for start in range(0, n_files, batch_size):\n",
    "        end = min(start + batch_size, n_files)\n",
    "        batch_files = filenames[start:end]\n",
    "        \n",
    "        batch_data = []\n",
    "        for fname in batch_files:\n",
    "            img = imread(os.path.join(img_dir, fname))\n",
    "            img = resize(img, target_shape, anti_aliasing=True)\n",
    "            batch_data.append(img)\n",
    "        \n",
    "        batch_array = np.array(batch_data)\n",
    "        \n",
    "        # Aquí puedes aplicar lo que necesites al batch\n",
    "        print(f\"Procesado batch {start}-{end} -> shape: {batch_array.shape}\")\n",
    "        \n",
    "        # Libera memoria después de procesar cada batch\n",
    "        del batch_data, batch_array\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14f7148f-518f-4502-be25-ae9b13d10c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "number_batch=500    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67fe34af-c5ff-4ad7-ad9a-c6fd1503a7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(80,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf1fffba-cf5c-4528-9b39-9ac1cabf6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  geo_train,geo_valid,geo_tests=split_data(list_of_paths[0],n_sample,n_train,n_valid,n_test) \n",
    "#  geo_array_train=arrays_img_processed(geo_train)\n",
    "def  process_by_batches(list_of_path,number_batch,option=True):\n",
    "    files=sorted([os.path.join(list_of_paths,fname) for fname in os.listdir(list_of_paths) if fname.endswith(\".png\")])\n",
    "    n_files=len(files)\n",
    "    for start in range(0,n_files,number_batch):\n",
    "         data_batch=[]\n",
    "         end = min(start + number_batch, n_files)\n",
    "         file_batches=files[start:end]\n",
    "         for img in file_batches:\n",
    "             x = processing(img,option)\n",
    "             data_batch.append(x)\n",
    "         del data_batch\n",
    "         gc.collect()\n",
    "             \n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd3412-54eb-43ff-b829-f007d159f52b",
   "metadata": {},
   "source": [
    "### 4. Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ffc45-ae43-4ce3-ba44-e1d90c0f7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method for reading image    \n",
    "def get_img(img_name):\n",
    "    return ski.io.imread(img_name)\n",
    "\n",
    "# method for turning to a grey or binary image \n",
    "def processing(image,option=True):\n",
    "        x = get_img(image)  \n",
    "        x = rgb2gray(x)       # It returns a grayscale image with floating point values in the range from 0 to 1\n",
    "        x =cv.resize(x,(w,h)) # Reshape image \n",
    "    \n",
    "        # Binary option otherwise gray\n",
    "        if (option):\n",
    "            x=ski.util.img_as_ubyte(x)  # Convert it back to the original data type and the data range back 0 to 255. \n",
    "                                        # It is often better to use image values represented by floating point values \n",
    "            best_value_threshold=np.round(filters.threshold_otsu(x)) #  Otsu’s method calculates an “optimal” threshold\n",
    "\n",
    "            _,x= cv.threshold(x, best_value_threshold, 255, cv.THRESH_BINARY)\n",
    "            x=x/255.\n",
    "        x=x[:, :, np.newaxis]\n",
    "        return x\n",
    "\n",
    "def arrays_img_processed(data,type=True): # True for Geometry \n",
    "    # begin in data[1:] \n",
    "    if type:\n",
    "        list_arrays=[processing(image,option=True).astype(np.float32) for image in data[1:] ]\n",
    "    else:\n",
    "        list_arrays=[processing(image,option=False).astype(np.float32) for image in data[1:] ]\n",
    "   \n",
    "    return np.array(list_arrays) \n",
    "    \n",
    "# Create a mask\n",
    "def create_mask(u,v):\n",
    "    return u*v\n",
    "\n",
    "# method for splitting data\n",
    "def split_data(path,n_sample,n_train,n_valid,n_test):\n",
    "        \n",
    "        files=sorted([os.path.join(path, fname) for fname in os.listdir(path) if fname.endswith(\".png\")])\n",
    "        n_files=len(files)\n",
    "        \n",
    "        if(n_files>=n_sample):\n",
    "                print('Loaded ',n_sample,'/',n_files,'**** from ',path,'\\n')\n",
    "                if n_valid == 0.:\n",
    "                    if (n_train + n_test == 1.0):\n",
    "                        n_train= math.floor(n_sample*n_train)\n",
    "                        n_test = math.floor(n_sample*n_test) + n_train\n",
    "                        (files_train,files_tests) = (files[0:n_train], files[n_train:n_test])\n",
    "                        return files_train,files_tests\n",
    "    \n",
    "                else:     \n",
    "                    if (n_train + n_valid + n_test == 1.0):\n",
    "        \n",
    "                        n_train= math.floor(n_sample*n_train)\n",
    "                        n_valid = math.floor(n_sample*n_valid) + n_train\n",
    "                        n_test = math.floor(n_sample*n_test) + n_valid\n",
    "              \n",
    "                        (files_train,files_valid,files_tests) = (files[0:n_train], files[n_train:n_valid],files[n_valid:n_test])\n",
    "                    \n",
    "                        return files_train,files_valid,files_tests\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30e785-3244-4ea1-9c9c-cbecd2857fdc",
   "metadata": {},
   "source": [
    "#### 4.1 Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a1fec-aca3-4045-b819-d3f87a8a1be3",
   "metadata": {},
   "source": [
    "#### 4.2 Dataset Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46f04e-2c2f-4df8-85fc-3915534fd4b5",
   "metadata": {},
   "source": [
    "#### 4.2 Create a dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36703ebe-69a2-4d10-9ef7-f1044ffb8de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_valid != 0.:    \n",
    "    \n",
    "        geo_train,geo_valid,geo_tests=split_data(list_of_paths[0],n_sample,n_train,n_valid,n_test)    # Geometry\n",
    "        mag_train,mag_valid,mag_tests=split_data(list_of_paths[1],n_sample,n_train,n_valid,n_test)   # Magnitude\n",
    "        p_train,p_valid,p_tests=split_data(list_of_paths[2],n_sample,n_train,n_valid,n_test)        # Pression\n",
    "        vx_train,vx_valid,vx_tests=split_data(list_of_paths[3],n_sample,n_train,n_valid,n_test)    # Vx\n",
    "        vy_train,vy_valid,vy_tests=split_data(list_of_paths[4],n_sample,n_train,n_valid,n_test)   # Vy\n",
    "\n",
    "        geo_array_train=arrays_img_processed(geo_train)\n",
    "        geo_array_valid=arrays_img_processed(geo_valid)\n",
    "        geo_array_tests=arrays_img_processed(geo_tests)\n",
    "\n",
    "        mag_array_train=arrays_img_processed(mag_train,False)\n",
    "        mag_array_valid=arrays_img_processed(mag_valid,False)\n",
    "        mag_array_tests=arrays_img_processed(mag_tests,False)\n",
    "\n",
    "        p_array_train=arrays_img_processed(p_train,False)\n",
    "        p_array_valid=arrays_img_processed(p_valid,False)\n",
    "        p_array_tests=arrays_img_processed(p_tests,False)\n",
    "\n",
    "        vx_array_train=arrays_img_processed(vx_train,False)\n",
    "        vx_array_valid=arrays_img_processed(vx_valid,False)\n",
    "        vx_array_tests=arrays_img_processed(vx_tests,False)\n",
    "\n",
    "        vy_array_train=arrays_img_processed(vy_train,False)\n",
    "        vy_array_valid=arrays_img_processed(vy_valid,False)\n",
    "        vy_array_tests=arrays_img_processed(vy_tests,False)\n",
    "\n",
    "        mag_masked_train = create_mask(geo_array_train,mag_array_train)\n",
    "        mag_masked_valid = create_mask(geo_array_valid,mag_array_valid)\n",
    "        mag_masked_test  = create_mask(geo_array_test,mag_array_test)\n",
    "\n",
    "        p_masked_train = create_mask(geo_array_train,p_array_train)\n",
    "        p_masked_valid = create_mask(geo_array_valid,p_array_valid)\n",
    "        p_masked_test =  create_mask(geo_array_test,p_array_test)\n",
    "\n",
    "        vx_masked_train = create_mask(geo_array_train,vx_array_train)\n",
    "        vx_masked_valid = create_mask(geo_array_train,vx_array_train)\n",
    "        vx_masked_test =  create_mask(geo_array_train,vx_array_train)\n",
    "\n",
    "        vy_masked_train = create_mask(geo_array_train,vy_array_train)\n",
    "        vy_masked_valid = create_mask(geo_array_train,vy_array_train)\n",
    "        vy_masked_test =  create_mask(geo_array_train,vy_array_train)\n",
    "\n",
    "else:\n",
    "        geo_train, geo_tests=split_data(list_of_paths[0],n_sample,n_train,n_valid,n_test)\n",
    "        mag_train,mag_tests=split_data(list_of_paths[1],n_sample,n_train,n_valid,n_test)\n",
    "        p_train,p_tests=split_data(list_of_paths[2],n_sample,n_train,n_valid,n_test)\n",
    "        vx_train,vx_tests=split_data(list_of_paths[3],n_sample,n_train,n_valid,n_test)\n",
    "        vy_train,vy_tests=split_data(list_of_paths[4],n_sample,n_train,n_valid,n_test)\n",
    "\n",
    "        geo_array_train=arrays_img_processed(geo_train)\n",
    "        geo_array_tests=arrays_img_processed(geo_tests)\n",
    "\n",
    "        mag_array_train=arrays_img_processed(mag_train,False)\n",
    "        mag_array_tests=arrays_img_processed(mag_tests,False)\n",
    "\n",
    "        p_array_train=arrays_img_processed(p_train,False)\n",
    "        p_array_tests=arrays_img_processed(p_tests,False)\n",
    "\n",
    "        vx_array_train=arrays_img_processed(vx_train,False)\n",
    "        vx_array_tests=arrays_img_processed(vx_tests,False)\n",
    "\n",
    "        vy_array_train=arrays_img_processed(vy_train,False)\n",
    "        vy_array_tests=arrays_img_processed(vy_tests,False)\n",
    "        # *******************************************************************\n",
    "        mag_masked_train = create_mask(geo_array_train,mag_array_train)\n",
    "        mag_masked_test  = create_mask(geo_array_tests,mag_array_tests)\n",
    "\n",
    "        p_masked_train = create_mask(geo_array_train,p_array_train)\n",
    "        p_masked_test =  create_mask(geo_array_tests,p_array_tests)\n",
    "\n",
    "        vx_masked_train = create_mask(geo_array_train,vx_array_train)\n",
    "        vx_masked_test =  create_mask(geo_array_tests,vx_array_tests)\n",
    "    \n",
    "        vy_masked_train = create_mask(geo_array_train,vy_array_train)\n",
    "        vy_masked_test =  create_mask(geo_array_tests,vy_array_tests)\n",
    "\n",
    "print(f'Training data for geometry: {geo_array_train.shape}')\n",
    "print(f'Test data for geometry: {geo_array_tests.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a7eaa-f8a0-4b74-9964-1f95b61bdc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor of four channels\n",
    "data_merged_train = np.concatenate((mag_masked_train, p_masked_train, vx_masked_train, vy_masked_train), axis=-1)\n",
    "data_merged_tests = np.concatenate((mag_masked_test,p_masked_test,vx_masked_test, vy_masked_test), axis=-1)\n",
    "\n",
    "# slices of an array in the form of objects by using tf.data.Dataset.from_tensor_slices() method\n",
    "train_data=tf.data.Dataset.from_tensor_slices((geo_array_train, data_merged_train))\n",
    "tests_data=tf.data.Dataset.from_tensor_slices((geo_array_tests, data_merged_tests))\n",
    "\n",
    "print(data_merged_train.shape)\n",
    "print(data_merged_tests.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2091e-6c23-48e6-b71d-f2197b79c78b",
   "metadata": {},
   "source": [
    "#### 4.3 Visualizing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f97684e-93fc-4862-ab1d-511a5d00a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img1,img2,img3,img4,title=None,opt=None,prediction=None):\n",
    "    if opt==None and prediction==None:\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 5), sharey=True)\n",
    "        axes[0, 0].set_xticks([]);axes[0, 0].set_yticks([]);\n",
    "        axes[1, 0].set_xticks([]); axes[1, 0].set_yticks([]);\n",
    "        axes[0, 1].set_xticks([]); axes[0, 1].set_yticks([]);\n",
    "        axes[1, 1].set_xticks([]); axes[1, 1].set_yticks([]);\n",
    "        \n",
    "        axes[0, 0].imshow(img1,origin='upper',cmap='gray')\n",
    "        axes[0, 0].set_title('Input',fontsize=14)\n",
    "        axes[0, 0].set_ylabel('Geometry',fontsize=14)\n",
    "\n",
    "        axes[0, 1].imshow(img2,origin='upper',cmap='gray')\n",
    "        axes[0, 1].set_title('Output-Magnitude',fontsize=14)\n",
    "        axes[0, 1].set_ylabel('V',fontsize=14)\n",
    "\n",
    "        axes[1, 0].imshow(img3,origin='upper',cmap='gray')\n",
    "        axes[1, 0].set_title('Output x-velocity',fontsize=14)\n",
    "        axes[1, 0].set_ylabel('Vx',fontsize=14)\n",
    "\n",
    "        axes[1, 1].imshow(img4,origin='upper',cmap='gray')\n",
    "        axes[1, 1].set_title('Output-Pression',fontsize=14)\n",
    "        axes[1, 1].set_ylabel('P',fontsize=14)\n",
    "\n",
    "        if title is not None: fig.suptitle(title)\n",
    "    \n",
    "    else:\n",
    "        fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(16, 5), sharey=True)\n",
    "        ax1.set_xticks([]); ax1.set_yticks([]);\n",
    "        ax2.set_xticks([]); ax2.set_yticks([]);\n",
    "        ax1.imshow(a1,origin='upper',cmap='gray')\n",
    "        ax1.set_title('Real',fontsize=14)\n",
    "        #ax1.set_xlabel('Geometry')\n",
    "        ax1.set_ylabel('u',fontsize=14)\n",
    "        ax2.imshow(a2,origin='upper',cmap='gray')\n",
    "        ax2.set_title('Predicted',fontsize=14)\n",
    "        #ax2.set_xlabel('u')\n",
    "        ax2.set_ylabel(prediction,fontsize=16)\n",
    "        if title is not None: fig.suptitle(title)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2e293-2055-42fd-b241-08827b36d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_random=random.sample(range(1, geo_array_train.shape[0]), 1)\n",
    "show_image(geo_array_train[img_random[0]],mag_masked_train[img_random[0]],\n",
    "           vx_masked_train[img_random[0]],p_masked_train[img_random[0]],\n",
    "           title=f\"Training data sample N°{img_random[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a94d3-3503-4e7a-a8bb-cfff575f76ea",
   "metadata": {},
   "source": [
    "### 5. Setup Hiperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b5f37-0752-49ba-a1ed-9a1ad7d0bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes =param.num_classes\n",
    "num_epochs =param.num_epochs\n",
    "batch_size=param.batch_size\n",
    "patience=param.patience\n",
    "LR=param.LR\n",
    "print(f'Classes= {num_classes},\\n Epochs = {num_epochs},\\n batches = {batch_size},\\n lr = {LR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1dc430-e184-4c1d-9d19-79f994ca188b",
   "metadata": {},
   "source": [
    "### 6. Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee26e6-4345-4e91-82c4-2d10695dd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "inputs = Input(train_geometries.shape[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
