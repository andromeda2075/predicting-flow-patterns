{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import datetime\n",
    "import tensorflow                       as tf\n",
    "from tensorflow.keras.callbacks         import TensorBoard\n",
    "from tensorflow.keras.layers            import Input, Lambda,UpSampling2D, Conv2D,Dropout,MaxPooling2D,Conv2DTranspose,concatenate,BatchNormalization, Activation\n",
    "from tensorflow.keras.models            import Model\n",
    "from tensorflow.keras.optimizers        import Adam,RMSprop\n",
    "from keras.utils                        import plot_model\n",
    "from tensorflow.keras                   import layers, models\n",
    "from tensorflow.keras.losses            import mae\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import random, time\n",
    "from pathlib                        import Path\n",
    "from PIL                            import Image\n",
    "\n",
    "import skimage                      as ski\n",
    "from   skimage.filters              import threshold_otsu\n",
    "from   skimage                      import io, color\n",
    "from   skimage.color                import rgb2gray\n",
    "from   skimage                      import filters\n",
    "import cv2                          as cv\n",
    "import matplotlib.pyplot            as plt \n",
    "import gc\n",
    "import glob\n",
    "from skimage                        import img_as_ubyte\n",
    "from skimage                        import io\n",
    "import shutil\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">1. Load dataset</font>\n",
    "\n",
    "Load NumPy arrays with tf.data.Dataset. Passing the two arrays as a tuple into tf.data.Dataset.from_tensor_slices to create a tf.data.Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_directory = '/home/ppgi/Trabajo/predicting-flow-patterns/P_Masked'\n",
    "\n",
    "path_train ='/home/ppgi/Trabajo/predicting-flow-patterns/P_Masked/train'\n",
    "path_test ='/home/ppgi/Trabajo/predicting-flow-patterns/P_Masked/test'\n",
    "path_val= '/home/ppgi/Trabajo/predicting-flow-patterns/P_Masked/valid'\n",
    "\n",
    "nbatch = 10\n",
    "\n",
    "def dataset_array(flow_path,geo_path):\n",
    "    def load_array(flow,geo):\n",
    "        x = np.load(flow)\n",
    "        y = np.load(geo)\n",
    "        return x,y\n",
    "\n",
    "    x, y = tf.numpy_function(load_array, [flow_path, geo_path], [tf.float64, tf.float64])\n",
    "    x.set_shape([x.shape[0],x.shape[1],x.shape[2]])\n",
    "    y.set_shape([y.shape[0],y.shape[1],y.shape[2]])\n",
    "    return x,y\n",
    "\n",
    "def create_dataset(flow_path,geo_path,batch_size = nbatch):\n",
    "    x_files = sorted(glob.glob(os.path.join(flow_path, \"*.npy\")))\n",
    "    y_files = sorted(glob.glob(os.path.join(geo_path, \"*.npy\")))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_files, y_files))\n",
    "    dataset = dataset.map(dataset_array, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_path_train='/home/ppgi/Trabajo/predicting-flow-patterns/G_Masked/train'\n",
    "p_path_train='/home/ppgi/Trabajo/predicting-flow-patterns/P_Masked/train'\n",
    "\n",
    "geo_path_valid='/home/ppgi/Trabajo/predicting-flow-patterns/G_Masked/valid'\n",
    "p_path_valid='/home/ppgi/Trabajo/predicting-flow-patterns/P_Masked/valid'\n",
    "\n",
    "geo_path_test='/home/ppgi/Trabajo/predicting-flow-patterns/G_Masked/test'\n",
    "p_path_test='/home/ppgi/Trabajo/predicting-flow-patterns/P_Masked/test'\n",
    "\n",
    "train_ds =create_dataset(geo_path_train,p_path_train)\n",
    "test_ds  =create_dataset(geo_path_test,p_path_test)\n",
    "valid_ds =create_dataset(geo_path_valid,p_path_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">3. Hiperparameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "patience=25  # How long to wait after last time validation loss improved\n",
    "LR=0.001\n",
    "# Model name\n",
    "model_name=\"U-Net\"\n",
    "save_in='/home/ppgi/Trabajo/predicting-flow-patterns'\n",
    "\n",
    "# image dimensions\n",
    "img_width             =  128   # 739   G:737\n",
    "img_height            =  32   # 185\n",
    "channel               =  1\n",
    "\n",
    "number_of_filters = [64, 128, 256, 512, 1024]\n",
    "\n",
    "type_padding = 'same'\n",
    "f_activation = 'relu'\n",
    "f_activation_last='relu'\n",
    "optimizer = RMSprop(learning_rate=LR)\n",
    "#optimizer = Adam(learning_rate=LR)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',  min_delta=1e-2, patience=patience, restore_best_weights=True,verbose=1)\n",
    "save = save_in + '/Results_weights/Best_weights.weights.h5'\n",
    "checkpoint_cb=tf.keras.callbacks.ModelCheckpoint(save,save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">4. Architecture</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block_batchnorm(filters,x):\n",
    "    conv = Conv2D(filters, (3, 3), padding=type_padding)(x)\n",
    "    conv= BatchNormalization()(conv)\n",
    "    conv = Activation(f_activation)(conv)\n",
    "    conv = Conv2D(filters, (3, 3), padding=type_padding)(conv)\n",
    "    conv= BatchNormalization()(conv)\n",
    "    conv = Activation(f_activation)(conv)\n",
    "    return conv\n",
    "    \n",
    "def conv_block(filters,x):\n",
    "    conv = Conv2D(filters, (3, 3), activation=f_activation, padding=type_padding)(x)\n",
    "    conv = Conv2D(filters, (3, 3), activation=f_activation, padding=type_padding)(conv)\n",
    "    return conv    \n",
    "\n",
    "def encoder(x,filters):\n",
    "    conv = conv_block_batchnorm(filters,x)\n",
    "    downsample = MaxPooling2D((2,2))(conv)\n",
    "    return conv,downsample\n",
    "\n",
    "def decoder(x1,x2,filters,transpose=None):\n",
    "    if transpose != None:\n",
    "        conv_up = Conv2DTranspose(filters,(2,2),strides=(2, 2),padding=type_padding)(x1)\n",
    "    else:\n",
    "        conv_up = UpSampling2D((2, 2))(x1)\n",
    "   \n",
    "    concat=concatenate([conv_up,x2],axis = 3)\n",
    "    up = conv_block(filters, concat)\n",
    "    return up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_of_filters = [64, 128, 256, 512, 1024]\n",
    "def make_model():\n",
    "    \n",
    "    image_input = Input((img_height, img_width, channel))\n",
    "    \n",
    "    conv1,down_block1 = encoder (image_input, number_of_filters[0])\n",
    "    conv2,down_block2 = encoder (down_block1 , number_of_filters[1])\n",
    "    conv3,down_block3 = encoder (down_block2 , number_of_filters[2])\n",
    "    conv4,down_block4 = encoder (down_block3 , number_of_filters[3])\n",
    "\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
